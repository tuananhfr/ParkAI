"""
Detection Service - Ch·∫°y AI detection trong thread ri√™ng
"""
import threading
import time
import numpy as np
from functools import lru_cache

import config


class Detection:
    """Detection object - Gi·ªëng demo code"""
    def __init__(self, coords, category, conf, metadata, imx500, picam2):
        self.category = category
        self.conf = conf
        # Use IMX500 convert_inference_coords nh∆∞ demo code
        self.box = imx500.convert_inference_coords(coords, metadata, picam2)


class DetectionService:
    """Service ch·∫°y AI detection"""
    
    def __init__(self, camera_manager, websocket_manager, ocr_service=None):
        self.camera_manager = camera_manager
        self.websocket_manager = websocket_manager
        self.ocr_service = ocr_service
        
        self.intrinsics = camera_manager.get_intrinsics()
        self.imx500 = camera_manager.get_imx500()
        self.picam2 = camera_manager.get_picam2()
        
        self.running = False
        self.detection_thread = None
        
        # Stats
        self.total_detections = 0
        self.total_frames = 0
        self.fps = 0
        self.last_fps_time = time.time()
        self.frames_in_second = 0

        # OCR throttling - ch·ªâ ch·∫°y m·ªói N frames
        self.ocr_frame_skip = config.OCR_FRAME_SKIP
        self.last_ocr_results = {}  # Cache OCR results by bbox

        # Statistics for debugging
        self.outputs_success = 0
        self.outputs_fail = 0

        # VEHICLE TRACKING - tr√°nh duplicate detections cho c√πng 1 xe
        self.current_vehicle = None  # {'text': str, 'confidence': float, 'first_seen': time, 'last_seen': time, 'best_confidence': float}
        self.vehicle_timeout = 10.0  # Sau 10s kh√¥ng th·∫•y xe, cho ph√©p xe m·ªõi v√†o
        self.similarity_threshold = 0.7  # N·∫øu text gi·ªëng > 70%, coi l√† c√πng xe

    def start(self):
        """B·∫Øt ƒë·∫ßu detection thread"""
        if self.running:
            return
        
        self.running = True
        self.detection_thread = threading.Thread(target=self._detection_loop, daemon=True)
        self.detection_thread.start()
    
    def stop(self):
        """D·ª´ng detection"""
        self.running = False

        if self.detection_thread:
            self.detection_thread.join(timeout=2)


    def _detection_loop(self):
        """Loop detection - T·∫¨N D·ª§NG IMX500, CH·ªà PARSE METADATA"""

        while self.running:
            try:
                frame_data = self.camera_manager.get_frame_for_detection()

                if frame_data is None:
                    continue

                # OPTIMIZATION: Kh√¥ng c·∫ßn frame - IMX500 ƒë√£ c√≥ bbox trong metadata
                metadata = frame_data['metadata']
                timestamp = frame_data['timestamp']
                frame_id = frame_data['frame_id']
                cached_outputs = frame_data.get('outputs')  # Get cached outputs (n·∫øu c√≥)

                # Parse detections t·ª´ IMX500 metadata (ƒë√£ c√≥ bbox s·∫µn)
                detections = self._parse_detections(metadata, cached_outputs)

                # Update stats
                self.total_frames += 1
                self.frames_in_second += 1
                current_time = time.time()

                if current_time - self.last_fps_time >= 1.0:
                    self.fps = self.frames_in_second
                    self.frames_in_second = 0
                    self.last_fps_time = current_time


                # Convert detections v√† run OCR n·∫øu enabled
                detection_results = []
                frame = frame_data.get('frame')

                # Check xem frame n√†y c√≥ ch·∫°y OCR kh√¥ng
                should_run_ocr = (self.total_frames % self.ocr_frame_skip == 0)

                for detection in detections:
                    # Detection object c√≥ .box (x, y, w, h) format
                    x, y, w, h = detection.box
                    category_idx = int(detection.category)
                    confidence = float(detection.conf)

                    labels = self._get_labels()
                    label = labels[category_idx] if category_idx < len(labels) else f"Class_{category_idx}"

                    detection_dict = {
                        'class': label,
                        'confidence': confidence,
                        'bbox': [int(x), int(y), int(w), int(h)],
                        'timestamp': timestamp,
                        'frame_id': frame_id
                    }

                    # Run OCR - CH·ªà M·ªñI N FRAMES ƒë·ªÉ gi·∫£m lag
                    if config.ENABLE_OCR and self.ocr_service and self.ocr_service.is_ready() and frame is not None:
                        # T·∫°o bbox key ƒë·ªÉ cache
                        bbox_key = (x, y, w, h)

                        if should_run_ocr:
                            try:
                                # Validate bbox trong frame boundaries
                                frame_h, frame_w = frame.shape[:2]
                                x_valid = max(0, min(x, frame_w - 1))
                                y_valid = max(0, min(y, frame_h - 1))
                                w_valid = min(w, frame_w - x_valid)
                                h_valid = min(h, frame_h - y_valid)

                                # Crop detection region
                                if w_valid > 10 and h_valid > 10:  # Ch·ªâ crop n·∫øu ƒë·ªß l·ªõn
                                    crop = frame[y_valid:y_valid+h_valid, x_valid:x_valid+w_valid]

                                    if crop.size > 0:
                                        text = self.ocr_service.recognize(crop)
                                        if text:
                                            # L·ªåC PATTERN BI·ªÇN S·ªê VI·ªÜT NAM
                                            if self._is_valid_vietnamese_plate(text):
                                                # VEHICLE TRACKING LOGIC
                                                is_new_vehicle = self._process_vehicle_detection(text, confidence, current_time)

                                                # Ch·ªâ th√™m text v√†o detection n·∫øu l√† xe m·ªõi ho·∫∑c update xe hi·ªán t·∫°i
                                                if is_new_vehicle or self.current_vehicle:
                                                    detection_dict['text'] = text
                                                    self.last_ocr_results[bbox_key] = text
                                            else:
                                                print(f"‚ö†Ô∏è  Filtered invalid plate: '{text}'")
                            except Exception as e:
                                print(f"‚ùå OCR error: {e}")
                        else:
                            # D√πng cached result n·∫øu c√≥
                            if bbox_key in self.last_ocr_results:
                                detection_dict['text'] = self.last_ocr_results[bbox_key]

                    detection_results.append(detection_dict)

                if len(detection_results) > 0:
                    self.total_detections += len(detection_results)

                    # G·ª¨I M·ªåI FRAME C√ì DETECTION (ƒë·ªÉ boxes hi·ªÉn th·ªã li√™n t·ª•c)
                    self.websocket_manager.broadcast_detections(detection_results)

                    # Log ƒë∆∞·ª£c x·ª≠ l√Ω trong _process_vehicle_detection r·ªìi, kh√¥ng c·∫ßn log l·∫°i ·ªü ƒë√¢y

                # Cleanup OCR cache m·ªói 100 frames ƒë·ªÉ tr√°nh memory leak
                if self.total_frames % 100 == 0:
                    self.last_ocr_results.clear()

            except Exception as e:
                print(f"‚ùå Error: {e}")
                import traceback
                traceback.print_exc()
                time.sleep(0.1)

    def _parse_detections(self, metadata, cached_outputs=None):
        """Parse detections t·ª´ IMX500 - Gi·ªëng logic demo code"""
        try:
            # Use cached outputs t·ª´ camera_manager (n·∫øu c√≥) ƒë·ªÉ tr√°nh g·ªçi get_outputs 2 l·∫ßn
            if cached_outputs is not None:
                outputs = cached_outputs
            else:
                # Fallback: extract t·ª´ metadata (legacy path)
                outputs = self.imx500.get_outputs(metadata, add_batch=True)

            # Track statistics
            if outputs is None or len(outputs) < 3:
                self.outputs_fail += 1
                return []

            # Success
            self.outputs_success += 1

            # Check if model has postprocess built-in
            if self.intrinsics.postprocess == "nanodet":
                # Use nanodet postprocessing (nh∆∞ demo code)
                from picamera2.devices.imx500 import postprocess_nanodet_detection
                from picamera2.devices.imx500.postprocess import scale_boxes

                input_w, input_h = self.imx500.get_input_size()
                boxes, scores, classes = postprocess_nanodet_detection(
                    outputs=outputs[0],
                    conf=config.DETECTION_THRESHOLD,
                    iou_thres=config.IOU_THRESHOLD,
                    max_out_dets=config.MAX_DETECTIONS
                )[0]
                boxes = scale_boxes(boxes, 1, 1, input_h, input_w, False, False)
            else:
                # Model c√≥ built-in postprocessing - THEO ƒê√öNG DEMO CODE
                boxes = outputs[0][0]      # Shape: (300, 4)
                scores = outputs[1][0]     # Shape: (300,)
                classes = outputs[2][0]    # Shape: (300,)

                # Normalize ƒê√öNG nh∆∞ demo code
                if self.intrinsics.bbox_normalization:
                    input_w, input_h = self.imx500.get_input_size()
                    boxes = boxes / input_h  # CH·ªà chia cho HEIGHT

                # Swap bbox order ƒê√öNG nh∆∞ demo code
                if self.intrinsics.bbox_order == "xy":
                    boxes = boxes[:, [1, 0, 3, 2]]  # Swap y‚Üîx: (x,y,x,y) ‚Üí (y,x,y,x)

                # Reshape ƒê√öNG nh∆∞ demo code
                boxes = np.array_split(boxes, 4, axis=1)
                boxes = [arr.flatten() for arr in boxes]
                boxes = zip(*boxes)  # Generator, gi·ªëng demo code

            # Filter by threshold v√† create Detection objects
            detections = []
            for box, score, category in zip(boxes, scores, classes):
                if score > config.DETECTION_THRESHOLD:
                    # D√ôNG Detection class nh∆∞ demo code - convert_inference_coords() s·∫Ω handle ƒë√∫ng!
                    detection = Detection(box, category, score, metadata, self.imx500, self.picam2)

                    # Get bbox ƒë√£ ƒë∆∞·ª£c convert sang pixel coords
                    x, y, w, h = detection.box

                    # Calculate aspect ratio
                    aspect_ratio = w / h if h > 0 else 0

                    # Debug: Collect info
                    # Bi·ªÉn s·ªë Vi·ªát Nam: width/height th∆∞·ªùng 2.0 - 4.5
                    # Filter: ch·ªâ accept boxes N·∫∞M NGANG
                    if aspect_ratio > config.MIN_PLATE_ASPECT_RATIO:
                        detections.append(detection)

            return detections

        except Exception as e:
            # LU√îN log exception ƒë·∫ßu ti√™n, sau ƒë√≥ m·ªói 50 frames
            if not hasattr(self, '_parse_error_logged') or self.total_frames % 50 == 0:
                print(f"‚ùå Parse detection error: {e}")
                import traceback
                traceback.print_exc()
                self._parse_error_logged = True
            return []

    def _process_vehicle_detection(self, text, confidence, current_time):
        """
        X·ª≠ l√Ω vehicle tracking - tr√°nh duplicate detections

        Return: True n·∫øu l√† xe m·ªõi (c·∫ßn log/broadcast), False n·∫øu l√† xe c≈©
        """
        # N·∫øu kh√¥ng c√≥ xe n√†o ƒëang track
        if self.current_vehicle is None:
            # Xe m·ªõi v√†o
            self.current_vehicle = {
                'text': text,
                'confidence': confidence,
                'first_seen': current_time,
                'last_seen': current_time,
                'best_confidence': confidence
            }
            print(f"üöó NEW VEHICLE: {text} (confidence: {confidence:.2f})")
            return True

        # ƒê√£ c√≥ xe ƒëang track
        time_since_last_seen = current_time - self.current_vehicle['last_seen']

        # Check timeout - n·∫øu qu√° 10s kh√¥ng th·∫•y xe, xe ƒë√£ ƒëi
        if time_since_last_seen > self.vehicle_timeout:
            # Xe c≈© ƒë√£ ƒëi, xe m·ªõi v√†o
            print(f"‚è±Ô∏è  Vehicle timeout ({time_since_last_seen:.1f}s), accepting new vehicle")
            self.current_vehicle = {
                'text': text,
                'confidence': confidence,
                'first_seen': current_time,
                'last_seen': current_time,
                'best_confidence': confidence
            }
            print(f"üöó NEW VEHICLE: {text} (confidence: {confidence:.2f})")
            return True

        # Xe v·∫´n trong view - check similarity
        similarity = self._levenshtein_similarity(text, self.current_vehicle['text'])

        if similarity >= self.similarity_threshold:
            # C√πng xe - update last_seen
            self.current_vehicle['last_seen'] = current_time

            # Update text n·∫øu confidence cao h∆°n
            if confidence > self.current_vehicle['best_confidence']:
                old_text = self.current_vehicle['text']
                self.current_vehicle['text'] = text
                self.current_vehicle['confidence'] = confidence
                self.current_vehicle['best_confidence'] = confidence
                print(f"üìù UPDATE: {old_text} ‚Üí {text} (confidence: {confidence:.2f}, similarity: {similarity:.2%})")
            else:
                # Confidence th·∫•p h∆°n, gi·ªØ nguy√™n text c≈©
                print(f"üìå KEEP: {self.current_vehicle['text']} (new reading: {text}, similarity: {similarity:.2%})")

            return False  # Kh√¥ng ph·∫£i xe m·ªõi
        else:
            # Text kh√°c xa (< 70% similar) - CH·ªú TH√äM ƒê·ªÇ CH·∫ÆC CH·∫ÆN
            # C√≥ th·ªÉ l√†: (1) OCR ƒë·ªçc sai ho·∫∑c (2) xe kh√°c
            # N·∫øu ƒë√£ th·∫•y xe c≈© trong v√≤ng 5s g·∫ßn ƒë√¢y, ignore reading m·ªõi (coi nh∆∞ OCR sai)
            time_in_view = current_time - self.current_vehicle['first_seen']

            if time_in_view < 5.0:
                # Xe m·ªõi v√†o ch∆∞a l√¢u, c√≥ th·ªÉ OCR ƒëang ·ªïn ƒë·ªãnh ‚Üí ignore
                print(f"‚ö†Ô∏è  IGNORE: {text} (similarity: {similarity:.2%} with {self.current_vehicle['text']}, vehicle still in view)")
                return False
            else:
                # Xe ƒë√£ ·ªü trong view > 5s, reading m·ªõi qu√° kh√°c ‚Üí c√≥ th·ªÉ l√† xe m·ªõi
                # Accept nh∆∞ xe m·ªõi
                print(f"üîÑ REPLACE: {self.current_vehicle['text']} ‚Üí {text} (similarity: {similarity:.2%}, long time in view)")
                self.current_vehicle = {
                    'text': text,
                    'confidence': confidence,
                    'first_seen': current_time,
                    'last_seen': current_time,
                    'best_confidence': confidence
                }
                return True

    def _levenshtein_similarity(self, s1, s2):
        """
        T√≠nh similarity gi·ªØa 2 string b·∫±ng Levenshtein distance
        Return: 0.0 - 1.0 (1.0 = ho√†n to√†n gi·ªëng)
        """
        if s1 == s2:
            return 1.0

        if len(s1) == 0 or len(s2) == 0:
            return 0.0

        # Levenshtein distance algorithm
        len1, len2 = len(s1), len(s2)
        distances = [[0] * (len2 + 1) for _ in range(len1 + 1)]

        for i in range(len1 + 1):
            distances[i][0] = i
        for j in range(len2 + 1):
            distances[0][j] = j

        for i in range(1, len1 + 1):
            for j in range(1, len2 + 1):
                cost = 0 if s1[i-1] == s2[j-1] else 1
                distances[i][j] = min(
                    distances[i-1][j] + 1,      # deletion
                    distances[i][j-1] + 1,      # insertion
                    distances[i-1][j-1] + cost  # substitution
                )

        # Convert distance to similarity (0-1)
        max_len = max(len1, len2)
        similarity = 1.0 - (distances[len1][len2] / max_len)
        return similarity

    def _is_valid_vietnamese_plate(self, text):
        """
        Ki·ªÉm tra text c√≥ ph√π h·ª£p v·ªõi format bi·ªÉn s·ªë Vi·ªát Nam kh√¥ng

        Formats h·ª£p l·ªá:
        - 1 d√≤ng: 29A12345, 51F98765 (2-3 s·ªë + 1 ch·ªØ + 4-5 s·ªë)
        - 2 d√≤ng: 29A-12345, 51F-98765 (c√≥ d·∫•u -)
        """
        import re

        if not text or len(text) < 6:
            return False

        # Remove spaces v√† uppercase
        text = text.strip().upper().replace(" ", "")

        # Pattern bi·ªÉn s·ªë VN:
        # - B·∫Øt ƒë·∫ßu: 2-3 ch·ªØ s·ªë (m√£ t·ªânh)
        # - Theo sau: 1 ch·ªØ c√°i (A-Z, kh√¥ng c√≥ I, O, Q, W)
        # - K·∫øt th√∫c: 4-5 ch·ªØ s·ªë
        # - C√≥ th·ªÉ c√≥ d·∫•u - ·ªü gi·ªØa (bi·ªÉn 2 d√≤ng)

        patterns = [
            r'^\d{2}[A-HJ-NP-Z]\d{4,5}$',      # 29A12345
            r'^\d{2}[A-HJ-NP-Z]-?\d{4,5}$',    # 29A-12345
            r'^\d{3}[A-HJ-NP-Z]\d{4,5}$',      # 123A12345 (xe c√¥ng v·ª•)
            r'^\d{3}[A-HJ-NP-Z]-?\d{4,5}$',    # 123A-12345
        ]

        for pattern in patterns:
            if re.match(pattern, text):
                return True

        return False

    @lru_cache
    def _get_labels(self):
        """Get labels"""
        labels = self.intrinsics.labels
        if self.intrinsics.ignore_dash_labels:
            labels = [label for label in labels if label and label != "-"]
        return labels